{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb80d0fc2c884ba6888af0c91c1441ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883987f2161448f8a943ad520c85eb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ea23028706463c954bb64254bd7a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397a316efd4b4ffe974eb47adc2c0ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd2e8949d274bb3b81fa986e64394f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75794e9c2524f50803187c05d9f0b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-18 19:30:02,121] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 19:30:04.327277: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c07552c84142fb9017955eec467719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/291M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-33M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-33M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell me a story about a princess and a dragon. Do you want to hear a story about a princess and a dragon?\"\n",
      "\n",
      "Lily nodded. She liked stories. She liked princesses and dragons. She liked dragons and dragons. She said, \"Yes, please. Tell me a story about a princess and a dragon.\"\n",
      "\n",
      "Her mom smiled. She said, \"Okay. Let's go to the couch and I'll tell you a story about a princess and a dragon.\"\n",
      "\n",
      "Lily and her mom went to the couch. They cuddled and listened. Lily told her mom a story about a princess and a dragon. Her mom listened and nodded. She said, \"That's a good story. I like stories about princesses and dragons. They are beautiful and brave. Do you want to hear another one?\"\n",
      "\n",
      "Lily said, \"Yes, please. I want to hear another one.\"\n",
      "\n",
      "Her mom said, \"Okay. Let's hear another one.\"\n",
      "\n",
      "Lily and her mom cuddled and listened. They both felt happy and calm. They loved each other and their mom. They loved each other and their stories. They had a good day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"tell me a story\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "  output = model.generate(input_ids, max_length = 1000, num_beams=1)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trlx\n",
    "from trlx.data.default_configs import TRLConfig, default_ppo_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"method\": {\n",
      "        \"name\": \"PPOConfig\",\n",
      "        \"ppo_epochs\": 4,\n",
      "        \"num_rollouts\": 128,\n",
      "        \"chunk_size\": 128,\n",
      "        \"init_kl_coef\": 0.001,\n",
      "        \"target\": null,\n",
      "        \"horizon\": 10000,\n",
      "        \"gamma\": 1,\n",
      "        \"lam\": 0.95,\n",
      "        \"cliprange\": 0.2,\n",
      "        \"cliprange_value\": 0.2,\n",
      "        \"vf_coef\": 1,\n",
      "        \"scale_reward\": \"ignored\",\n",
      "        \"ref_mean\": null,\n",
      "        \"ref_std\": null,\n",
      "        \"cliprange_reward\": 10,\n",
      "        \"gen_kwargs\": {\n",
      "            \"max_new_tokens\": 40,\n",
      "            \"top_k\": 0,\n",
      "            \"top_p\": 1.0,\n",
      "            \"do_sample\": true\n",
      "        },\n",
      "        \"gen_experience_kwargs\": null\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"model_path\": \"roneneldan/TinyStories-33M\",\n",
      "        \"model_arch_type\": \"causal\",\n",
      "        \"num_layers_unfrozen\": 2,\n",
      "        \"delta_kwargs\": null\n",
      "    },\n",
      "    \"optimizer\": {\n",
      "        \"name\": \"adamw\",\n",
      "        \"kwargs\": {\n",
      "            \"lr\": 3e-05,\n",
      "            \"betas\": [\n",
      "                0.9,\n",
      "                0.95\n",
      "            ],\n",
      "            \"eps\": 1e-08,\n",
      "            \"weight_decay\": 1e-06\n",
      "        }\n",
      "    },\n",
      "    \"scheduler\": {\n",
      "        \"name\": \"cosine_annealing\",\n",
      "        \"kwargs\": {\n",
      "            \"T_max\": 1000000000000.0,\n",
      "            \"eta_min\": 3e-05\n",
      "        }\n",
      "    },\n",
      "    \"tokenizer\": {\n",
      "        \"tokenizer_path\": \"gpt2\",\n",
      "        \"padding_side\": \"left\",\n",
      "        \"truncation_side\": \"right\"\n",
      "    },\n",
      "    \"train\": {\n",
      "        \"total_steps\": 10000,\n",
      "        \"seq_length\": 1024,\n",
      "        \"epochs\": 100,\n",
      "        \"batch_size\": 32,\n",
      "        \"checkpoint_interval\": 10000,\n",
      "        \"eval_interval\": 100,\n",
      "        \"pipeline\": \"PromptPipeline\",\n",
      "        \"trainer\": \"AcceleratePPOTrainer\",\n",
      "        \"trainer_kwargs\": {},\n",
      "        \"project_name\": \"trlx\",\n",
      "        \"entity_name\": null,\n",
      "        \"group_name\": null,\n",
      "        \"checkpoint_dir\": \"ckpts\",\n",
      "        \"rollout_logging_dir\": null,\n",
      "        \"save_best\": true,\n",
      "        \"save_optimizer\": true,\n",
      "        \"tracker\": \"wandb\",\n",
      "        \"logging_dir\": null,\n",
      "        \"tags\": [],\n",
      "        \"seed\": 1000,\n",
      "        \"minibatch_size\": null\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "default_config = default_ppo_config().to_dict()\n",
    "#default_config['train']['tracker'] = None\n",
    "#default_config['train']['batch_size'] = 16\n",
    "#default_config['train']['epochs'] = 10\n",
    "default_config['model']['model_path'] = 'roneneldan/TinyStories-33M'\n",
    "#default_config['method']['gen_kwargs']['max_new_tokens'] = 46\n",
    "config = TRLConfig.update(default_config, {})\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(samples: List[str], **kwargs) -> Dict[str, List[float]]:\n",
    "  sentiments = list(map(get_positive_score, sentiment_fn(samples)))\n",
    "  return {\"sentiments\": sentiments}\n",
    "\n",
    "def reward_fn(input: Optional[Callable[[List[str], List[str], List[str]], List[float]]]):\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config, reward_fn, samples, eval_prompts, metric_fn, \n",
    "# prompts?\n",
    "trlx.train(config=config,\n",
    "           reward_fn=reward_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
